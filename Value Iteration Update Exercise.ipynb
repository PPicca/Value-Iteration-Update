{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An agent is trying to navigate a one-dimensional grid consisting of 5 cells. At each step, the agent has only one action to choose from, i.e. it moves to the cell on the immediate right.\n",
    "\n",
    "<b>Note:</b> The reward function is defined to be R(s,a,s′)=R(s), R(s=5)=1 and R(s)=0 otherwise. Note that we get the reward when we are leaving from the current state. When it reaches the rightmost cell, it stays for one more time step and then receives a reward of +1 and comes to a halt.\n",
    "\n",
    "Let V∗(i) denote the value function of state i, the ith cell starting from left.\n",
    "\n",
    "Let V∗k(i) denote the value function estimate at state i at the kth step of the value iteration algorithm. Let V∗0(i) denote the initialization of this estimate.\n",
    "\n",
    "Use the discount factor γ=0.5.\n",
    "\n",
    "We will write the functions V∗k as arrays below, i.e. as [V∗k(1)V∗k(2)V∗k(3)V∗k(4)V∗k(5)].\n",
    "\n",
    "Initialize by setting V∗0(i)=0 for all i:\n",
    "\n",
    " \tV∗0\t=\t[00000].\t \t \n",
    "Then, using the value iteration update rule, we get\n",
    "\n",
    " \tV∗1\t=\t[00001],\t \t \n",
    " \tV∗2\t=\t[0000.51]\t \t \n",
    "<b>Note:</b> Note that as soon as the agent takes the first action to reach cell 5, it stays for one more step and halts and does not take any more action, so we set V∗k+1(5)=V∗k(5) for all k≥1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 100\n",
    "gamma = 1/2\n",
    "V = np.zeros(5)\n",
    "\n",
    "R = np.array([[[0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0]],\n",
    "\n",
    "          [[0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0]],\n",
    "\n",
    "          [[0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0]],\n",
    "\n",
    "          [[0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0]],\n",
    "\n",
    "          [[1, 1, 1, 1, 1],\n",
    "           [1, 1, 1, 1, 1],\n",
    "           [1, 1, 1, 1, 1]]])\n",
    "\n",
    "# Matrix (5x3x5) for 5 states and 3 actions\n",
    "# a1 = stay\n",
    "# a2 = move right\n",
    "# a3 = move left\n",
    "\n",
    "\n",
    "T = np.array([[[1/2, 1/2, 0, 0, 0],\n",
    "           [2/3, 1/3, 0, 0, 0],\n",
    "           [1/2, 1/2, 0, 0, 0]],\n",
    "\n",
    "          [[1/4, 1/2, 1/4, 0, 0],\n",
    "           [0, 2/3, 1/3, 0, 0],\n",
    "           [1/3, 2/3, 0, 0, 0]],\n",
    "\n",
    "          [[0, 1/4, 1/2, 1/4, 0],\n",
    "           [0, 0, 2/3, 1/3, 0],\n",
    "           [0, 1/3, 2/3, 0, 0]],\n",
    "\n",
    "          [[0, 0, 1/4, 1/2, 1/4],\n",
    "           [0, 0, 0, 2/3, 1/3],\n",
    "           [0, 0, 1/3, 2/3, 0]],\n",
    "\n",
    "          [[0, 0, 0, 1/2, 1/2],\n",
    "           [0, 0, 0, 1/2, 1/2],\n",
    "           [0, 0, 0, 1/3, 2/3]]])\n",
    "\n",
    "for k in range(0,K):\n",
    "    Q = T * (R + gamma * V)\n",
    "    V = np.max(Q.sum(axis=2), axis = 1)\n",
    "    print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
