{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise part of MITx - Machine Learning with Python (Lecture 17 - Reinforcement Learning 1)\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall\n",
    "----\n",
    "\n",
    "An agent is trying to navigate a one-dimensional grid consisting of 5 cells. At each step, the agent has only one action to choose from, i.e. it moves to the cell on the immediate right.\n",
    "\n",
    "<b>Note:</b> The reward function is defined to be <b>R(s,a,s′) = R(s)</b>, <b>R(s=5) = 1</b> and <b>R(s)=0</b> otherwise. Note that we get the reward when we are leaving from the current state. When it reaches the rightmost cell, it stays for one more time step and then receives a reward of +1 and comes to a halt.\n",
    "\n",
    "Let <b>V_(i)</b> denote the value function of state i, the ith cell starting from left.\n",
    "\n",
    "Let <b>V_k(i)</b> denote the value function estimate at state i at the kth step of the value iteration algorithm. Let V∗0(i) denote the initialization of this estimate.\n",
    "\n",
    "Use the discount factor <b>γ=0.5</b>.\n",
    "\n",
    "We will write the functions V_k as arrays below, i.e. as [V∗k(1)V∗k(2)V∗k(3)V∗k(4)V∗k(5)].\n",
    "\n",
    "\n",
    "Initialize by setting V∗0(i)=0 for all i:<b>\n",
    "\n",
    "\n",
    "     V_0\t=\t[0 0 0 0 0] \t \n",
    "\n",
    "\n",
    "</b>Then, using the value iteration update rule, we get:<b>\n",
    "\n",
    " \tV_1\t=\t[0 0 0 0 1],\t \t \n",
    " \tV_2\t=\t[0 0 0 0.5 1]\t \t \n",
    "\n",
    "</b><b>Note:</b> Note that as soon as the agent takes the first action to reach cell 5, it stays for one more step and halts and does not take any more action, so we set V∗k+1(5)=V∗k(5) for all k≥1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "-----\n",
    "<b>(Changes to be made on the exercise proposed in the Lecture)</b>\n",
    "\n",
    "Consider the same one-dimensional grid with reward values as in the first few problems in this vertical. However, consider the following change to the transition probabilities: At any given grid location the agent can choose to either stay at the location or move to an adjacent grid location. If the agent chooses to stay at the location, such an action is successful with probability 1/2 and:\n",
    "\n",
    "- if the agent is at the leftmost or rightmost grid location it ends up at its neighboring grid location with probability 1/2,\n",
    "\n",
    "- if the agent is at any of the inner grid locations it has a probability 1/4 each of ending up at either of the neighboring locations.\n",
    "\n",
    "If the agent chooses to move (either left or right) at any of the inner grid locations, such an action is successful with probability 1/3 and with probability 2/3 it fails to move, and:\n",
    "\n",
    "- if the agent chooses to move left at the leftmost grid location, then the action ends up exactly the same as choosing to stay, i.e., staying at the leftmost grid location with probability 1/2, and ends up at its neighboring grid location with probability 1/2,\n",
    "\n",
    "- if the agent chooses to move right at the rightmost grid location, then the action ends up exactly the same as choosing to stay, i.e., staying at the rightmost grid location with probability 1/2, and ends up at its neighboring grid location with probability 1/2.\n",
    "\n",
    "Let <b>γ=0.5</b>.\n",
    "\n",
    "Run the value iteration algorithm for 100 iterations. Use any computational software of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 100\n",
    "gamma = 1/2\n",
    "V = np.zeros(5)\n",
    "\n",
    "R = np.array([[[0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0]],\n",
    "\n",
    "          [[0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0]],\n",
    "\n",
    "          [[0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0]],\n",
    "\n",
    "          [[0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0],\n",
    "           [0, 0, 0, 0, 0]],\n",
    "\n",
    "          [[1, 1, 1, 1, 1],\n",
    "           [1, 1, 1, 1, 1],\n",
    "           [1, 1, 1, 1, 1]]])\n",
    "\n",
    "# Matrix (5x3x5) for 5 states and 3 actions\n",
    "# a1 = stay\n",
    "# a2 = move right\n",
    "# a3 = move left\n",
    "\n",
    "\n",
    "T = np.array([[[1/2, 1/2, 0, 0, 0],\n",
    "           [2/3, 1/3, 0, 0, 0],\n",
    "           [1/2, 1/2, 0, 0, 0]],\n",
    "\n",
    "          [[1/4, 1/2, 1/4, 0, 0],\n",
    "           [0, 2/3, 1/3, 0, 0],\n",
    "           [1/3, 2/3, 0, 0, 0]],\n",
    "\n",
    "          [[0, 1/4, 1/2, 1/4, 0],\n",
    "           [0, 0, 2/3, 1/3, 0],\n",
    "           [0, 1/3, 2/3, 0, 0]],\n",
    "\n",
    "          [[0, 0, 1/4, 1/2, 1/4],\n",
    "           [0, 0, 0, 2/3, 1/3],\n",
    "           [0, 0, 1/3, 2/3, 0]],\n",
    "\n",
    "          [[0, 0, 0, 1/2, 1/2],\n",
    "           [0, 0, 0, 1/2, 1/2],\n",
    "           [0, 0, 0, 1/3, 2/3]]])\n",
    "\n",
    "for k in range(0,K):\n",
    "    Q = T * (R + gamma * V)\n",
    "    V = np.max(Q.sum(axis=2), axis = 1)\n",
    "    print(V)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
